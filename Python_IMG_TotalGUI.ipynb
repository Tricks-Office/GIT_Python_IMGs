{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\550sn\\AppData\\Local\\Temp\\ipykernel_17248\\2518366168.py:290: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_FileFolder.Item[idx] = var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 178 images belonging to 23 classes.\n",
      "Found 65 images belonging to 23 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\550sn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\550sn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 343ms/step - accuracy: 0.1102 - loss: 5.8820 - val_accuracy: 0.0625 - val_loss: 3.1311\n",
      "Epoch 2/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0625 - loss: 3.1195 - val_accuracy: 0.0000e+00 - val_loss: 3.0525\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\550sn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 265ms/step - accuracy: 0.0861 - loss: 3.1333 - val_accuracy: 0.0625 - val_loss: 3.0932\n",
      "Epoch 4/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0312 - loss: 3.1148 - val_accuracy: 0.0000e+00 - val_loss: 2.8197\n",
      "Epoch 5/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 258ms/step - accuracy: 0.0777 - loss: 3.0595 - val_accuracy: 0.0469 - val_loss: 3.0461\n",
      "Epoch 6/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.1562 - loss: 2.8752 - val_accuracy: 0.0000e+00 - val_loss: 3.0898\n",
      "Epoch 7/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 543ms/step - accuracy: 0.1697 - loss: 2.8667 - val_accuracy: 0.1562 - val_loss: 2.9274\n",
      "Epoch 8/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3438 - loss: 2.6968 - val_accuracy: 0.0000e+00 - val_loss: 3.3113\n",
      "Epoch 9/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 577ms/step - accuracy: 0.2865 - loss: 2.5342 - val_accuracy: 0.1406 - val_loss: 3.0314\n",
      "Epoch 10/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2188 - loss: 2.4182 - val_accuracy: 0.0000e+00 - val_loss: 3.8006\n",
      "Epoch 11/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 570ms/step - accuracy: 0.6135 - loss: 2.0435 - val_accuracy: 0.1406 - val_loss: 3.0468\n",
      "Epoch 12/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4444 - loss: 1.8272 - val_accuracy: 1.0000 - val_loss: 1.5074\n",
      "Epoch 13/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 528ms/step - accuracy: 0.6951 - loss: 1.3088 - val_accuracy: 0.2344 - val_loss: 2.9839\n",
      "Epoch 14/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6562 - loss: 1.2094 - val_accuracy: 1.0000 - val_loss: 1.7445\n",
      "Epoch 15/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 257ms/step - accuracy: 0.7527 - loss: 0.8530 - val_accuracy: 0.1406 - val_loss: 3.3643\n",
      "Epoch 16/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9062 - loss: 0.4258 - val_accuracy: 0.0000e+00 - val_loss: 2.7334\n",
      "Epoch 17/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 264ms/step - accuracy: 0.8994 - loss: 0.4226 - val_accuracy: 0.2031 - val_loss: 4.4259\n",
      "Epoch 18/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9444 - loss: 0.4146 - val_accuracy: 0.0000e+00 - val_loss: 1.6175\n",
      "Epoch 19/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 261ms/step - accuracy: 0.9238 - loss: 0.2326 - val_accuracy: 0.2188 - val_loss: 4.4350\n",
      "Epoch 20/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.1032 - val_accuracy: 1.0000 - val_loss: 0.0925\n",
      "Epoch 21/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 336ms/step - accuracy: 0.9921 - loss: 0.0712 - val_accuracy: 0.2031 - val_loss: 4.9558\n",
      "Epoch 22/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.0990 - val_accuracy: 0.0000e+00 - val_loss: 10.2227\n",
      "Epoch 23/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 305ms/step - accuracy: 0.9942 - loss: 0.0301 - val_accuracy: 0.2656 - val_loss: 5.5483\n",
      "Epoch 24/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0158 - val_accuracy: 0.0000e+00 - val_loss: 4.7154\n",
      "Epoch 25/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 546ms/step - accuracy: 1.0000 - loss: 0.0053 - val_accuracy: 0.2031 - val_loss: 6.0986\n",
      "Epoch 26/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0163 - val_accuracy: 0.0000e+00 - val_loss: 3.8780\n",
      "Epoch 27/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 538ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 0.2188 - val_loss: 5.9027\n",
      "Epoch 28/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0060 - val_accuracy: 0.0000e+00 - val_loss: 4.3998\n",
      "Epoch 29/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 540ms/step - accuracy: 1.0000 - loss: 0.0041 - val_accuracy: 0.2031 - val_loss: 5.8273\n",
      "Epoch 30/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 0.0022\n",
      "Epoch 31/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 316ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.2500 - val_loss: 5.7298\n",
      "Epoch 32/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 8.3292e-04 - val_accuracy: 0.0000e+00 - val_loss: 2.1962\n",
      "Epoch 33/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 267ms/step - accuracy: 1.0000 - loss: 9.3509e-04 - val_accuracy: 0.2188 - val_loss: 5.8200\n",
      "Epoch 34/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.0000e+00 - val_loss: 6.3368\n",
      "Epoch 35/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 255ms/step - accuracy: 1.0000 - loss: 8.2317e-04 - val_accuracy: 0.2344 - val_loss: 6.0280\n",
      "Epoch 36/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.7215e-04 - val_accuracy: 0.0000e+00 - val_loss: 3.5774\n",
      "Epoch 37/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 256ms/step - accuracy: 1.0000 - loss: 5.2405e-04 - val_accuracy: 0.2344 - val_loss: 6.0450\n",
      "Epoch 38/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.0017e-04 - val_accuracy: 0.0000e+00 - val_loss: 10.8869\n",
      "Epoch 39/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 264ms/step - accuracy: 1.0000 - loss: 3.3243e-04 - val_accuracy: 0.2656 - val_loss: 6.1483\n",
      "Epoch 40/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.9125e-04 - val_accuracy: 0.0000e+00 - val_loss: 10.3463\n",
      "Epoch 41/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 301ms/step - accuracy: 1.0000 - loss: 3.1228e-04 - val_accuracy: 0.2500 - val_loss: 6.3320\n",
      "Epoch 42/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.2181e-04 - val_accuracy: 0.0000e+00 - val_loss: 2.5147\n",
      "Epoch 43/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 286ms/step - accuracy: 1.0000 - loss: 2.1036e-04 - val_accuracy: 0.2500 - val_loss: 6.3925\n",
      "Epoch 44/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.5226e-04 - val_accuracy: 0.0000e+00 - val_loss: 1.2149\n",
      "Epoch 45/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 284ms/step - accuracy: 1.0000 - loss: 1.4778e-04 - val_accuracy: 0.2500 - val_loss: 6.3563\n",
      "Epoch 46/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.9610e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.7048\n",
      "Epoch 47/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 304ms/step - accuracy: 1.0000 - loss: 1.4921e-04 - val_accuracy: 0.2500 - val_loss: 6.0491\n",
      "Epoch 48/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.3645e-04 - val_accuracy: 0.0000e+00 - val_loss: 26.9555\n",
      "Epoch 49/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 319ms/step - accuracy: 1.0000 - loss: 9.8609e-05 - val_accuracy: 0.2344 - val_loss: 6.4911\n",
      "Epoch 50/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.6693e-04 - val_accuracy: 1.0000 - val_loss: 9.8581e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "Found 178 images belonging to 23 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\550sn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\tkinter\\__init__.py\", line 1968, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\550sn\\AppData\\Local\\Temp\\ipykernel_17248\\2518366168.py\", line 340, in <lambda>\n",
      "    btnPath.append(tk.Button(frameAnimalFace, text=\"Change Path\", width=10, padx =5, pady = 5, command=lambda i=i: onClick(i,fileyn[i])))\n",
      "                                                                                                                   ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\550sn\\AppData\\Local\\Temp\\ipykernel_17248\\2518366168.py\", line 302, in onClick\n",
      "    var = folder_selected.name\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 38 images belonging to 4 classes.\n",
      "Found 13 images belonging to 4 classes.\n",
      "Epoch 1/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.2812 - loss: 1.3997 - val_accuracy: 0.3077 - val_loss: 3.3188\n",
      "Epoch 2/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415ms/step - accuracy: 0.5000 - loss: 7.9759 - val_accuracy: 0.3077 - val_loss: 3.5923\n",
      "Epoch 3/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.3077 - val_loss: 3.5923\n",
      "Epoch 4/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.3333 - loss: 3.8137 - val_accuracy: 0.3077 - val_loss: 1.6858\n",
      "Epoch 5/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 495ms/step - accuracy: 0.2500 - loss: 1.6729 - val_accuracy: 0.0769 - val_loss: 1.4687\n",
      "Epoch 6/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0769 - val_loss: 1.4687\n",
      "Epoch 7/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.3750 - loss: 1.3655 - val_accuracy: 0.3846 - val_loss: 1.4530\n",
      "Epoch 8/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 410ms/step - accuracy: 0.3333 - loss: 1.2261 - val_accuracy: 0.3077 - val_loss: 1.4785\n",
      "Epoch 9/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.3077 - val_loss: 1.4785\n",
      "Epoch 10/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 1.1332 - val_accuracy: 0.2308 - val_loss: 1.5359\n",
      "Epoch 11/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471ms/step - accuracy: 0.5625 - loss: 1.3443 - val_accuracy: 0.2308 - val_loss: 1.3888\n",
      "Epoch 12/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 410ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.2308 - val_loss: 1.3888\n",
      "Epoch 13/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.6667 - loss: 1.1119 - val_accuracy: 0.3077 - val_loss: 1.3077\n",
      "Epoch 14/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.7188 - loss: 1.0012 - val_accuracy: 0.4615 - val_loss: 1.3477\n",
      "Epoch 15/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 627ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4615 - val_loss: 1.3477\n",
      "Epoch 16/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.6250 - loss: 0.9182 - val_accuracy: 0.2308 - val_loss: 1.4229\n",
      "Epoch 17/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 804ms/step - accuracy: 1.0000 - loss: 0.7104 - val_accuracy: 0.1538 - val_loss: 1.7016\n",
      "Epoch 18/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 611ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.1538 - val_loss: 1.7016\n",
      "Epoch 19/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.7188 - loss: 0.7389 - val_accuracy: 0.2308 - val_loss: 2.0180\n",
      "Epoch 20/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 771ms/step - accuracy: 0.5000 - loss: 0.9976 - val_accuracy: 0.3077 - val_loss: 2.0930\n",
      "Epoch 21/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 608ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.3077 - val_loss: 2.0930\n",
      "Epoch 22/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 0.7634 - val_accuracy: 0.3077 - val_loss: 1.8807\n",
      "Epoch 23/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 982ms/step - accuracy: 0.6875 - loss: 0.7809 - val_accuracy: 0.2308 - val_loss: 1.8536\n",
      "Epoch 24/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 559ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.2308 - val_loss: 1.8536\n",
      "Epoch 25/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.7500 - loss: 0.5997 - val_accuracy: 0.1538 - val_loss: 1.7707\n",
      "Epoch 26/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 744ms/step - accuracy: 0.6667 - loss: 0.7546 - val_accuracy: 0.1538 - val_loss: 1.7989\n",
      "Epoch 27/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 626ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.1538 - val_loss: 1.7989\n",
      "Epoch 28/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.8750 - loss: 0.5412 - val_accuracy: 0.2308 - val_loss: 1.8859\n",
      "Epoch 29/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 840ms/step - accuracy: 0.5000 - loss: 0.9360 - val_accuracy: 0.2308 - val_loss: 1.9563\n",
      "Epoch 30/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 588ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.2308 - val_loss: 1.9563\n",
      "Epoch 31/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.8333 - loss: 0.3489 - val_accuracy: 0.2308 - val_loss: 1.8805\n",
      "Epoch 32/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.9375 - loss: 0.3363 - val_accuracy: 0.2308 - val_loss: 1.8306\n",
      "Epoch 33/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 605ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.2308 - val_loss: 1.8306\n",
      "Epoch 34/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.2014 - val_accuracy: 0.4615 - val_loss: 1.9501\n",
      "Epoch 35/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 771ms/step - accuracy: 0.8333 - loss: 0.2996 - val_accuracy: 0.4615 - val_loss: 2.1773\n",
      "Epoch 36/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 641ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4615 - val_loss: 2.1773\n",
      "Epoch 37/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.9375 - loss: 0.2270 - val_accuracy: 0.2308 - val_loss: 2.2378\n",
      "Epoch 38/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 831ms/step - accuracy: 1.0000 - loss: 0.0444 - val_accuracy: 0.1538 - val_loss: 2.5702\n",
      "Epoch 39/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 606ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.1538 - val_loss: 2.5702\n",
      "Epoch 40/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 15s/step - accuracy: 1.0000 - loss: 0.0601 - val_accuracy: 0.1538 - val_loss: 3.1593\n",
      "Epoch 41/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 749ms/step - accuracy: 1.0000 - loss: 0.0934 - val_accuracy: 0.3077 - val_loss: 3.1022\n",
      "Epoch 42/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 652ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.3077 - val_loss: 3.1022\n",
      "Epoch 43/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0344 - val_accuracy: 0.3077 - val_loss: 3.3919\n",
      "Epoch 44/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0607 - val_accuracy: 0.3077 - val_loss: 3.4922\n",
      "Epoch 45/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 648ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.3077 - val_loss: 3.4922\n",
      "Epoch 46/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0636 - val_accuracy: 0.3077 - val_loss: 3.5465\n",
      "Epoch 47/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0064 - val_accuracy: 0.1538 - val_loss: 4.0081\n",
      "Epoch 48/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 611ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.1538 - val_loss: 4.0081\n",
      "Epoch 49/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 4.0805e-04 - val_accuracy: 0.2308 - val_loss: 4.8689\n",
      "Epoch 50/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 952ms/step - accuracy: 1.0000 - loss: 0.0110 - val_accuracy: 0.2308 - val_loss: 5.5742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Found 38 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from datetime import datetime\n",
    "\n",
    "### 기준정보로 등록해야 하는 정보\n",
    "## GUI 크기\n",
    "Gsize = \"620x480\"\n",
    "\n",
    "## 폴더 / 파일 리스트\n",
    "# 각 행의 라벨 정보 List로 정리\n",
    "label_f = ['Target folder', 'Result folder', 'Training folder','Model file','Test Image']\n",
    "# 각 행에서 다루는 값이 폴더일때는 0, 파일일때는 1로 구분자\n",
    "fileyn = [0, 0, 0, 1, 1]\n",
    "# 폴더 / 파일 리스트 기준정보 불러오기 (기준정보 관리 파일명 : GUIMaster.csv)\n",
    "# 기준정보 파일이 없을 경우 초기화\n",
    "try : \n",
    "    df_FileFolder= pd.read_csv(\"GUIMaster.csv\")\n",
    "except :\n",
    "    d = {'Item' : ['파일 / 폴더 경로를 설정해 주세요'] * len(label_f)}\n",
    "    df_FileFolder = pd.DataFrame(data=d)\n",
    "\n",
    "### 주요 함수\n",
    "## 개발된 함수 추가\n",
    "def apply_filter(filter_no):\n",
    "    target_folder = df_FileFolder.Item[0]\n",
    "    output_image_folder = df_FileFolder.Item[1]\n",
    "    \n",
    "    # 이미지 로드\n",
    "    for file in os.listdir(target_folder):\n",
    "        input_image_path = os.path.join(target_folder, file)\n",
    "        output_image_path = os.path.join(output_image_folder, file)\n",
    "        \n",
    "        img = cv2.imread(input_image_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "        # # 원본 이미지 출력\n",
    "        # plt.figure(figsize=(10, 10))\n",
    "        # plt.subplot(221), plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)), plt.title('Original Image')\n",
    "        match filter_no:\n",
    "            case 0 :\n",
    "                filter_result = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "                \n",
    "            case 1 :\n",
    "                filter_result = cv2.medianBlur(img, 5)\n",
    "\n",
    "            case 2 :\n",
    "                filter_result = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "                \n",
    "        cv2.imwrite(output_image_path, filter_result)\n",
    "\n",
    "def apply_edge(edge_no):\n",
    "        \n",
    "    # 폴더 경로 설정\n",
    "    target_folder = df_FileFolder.Item[0]\n",
    "    output_image_folder = df_FileFolder.Item[1]\n",
    "\n",
    "    for file in os.listdir(target_folder):\n",
    "        \n",
    "        # 이미지 로드\n",
    "        input_image_path = os.path.join(target_folder, file)\n",
    "        output_image_path = os.path.join(output_image_folder, file)\n",
    "\n",
    "        img = cv2.imread(input_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        match edge_no:\n",
    "            case 0 :\n",
    "                # Canny 엣지 검출\n",
    "                result_image = cv2.Canny(img, 100, 200)\n",
    "            case 1 : \n",
    "                ## Contour Detection\n",
    "                # 이진화\n",
    "                ret, thresh = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "                # 윤곽선 찾기\n",
    "                contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "                # 새로운 이미지에 윤곽선 그리기\n",
    "                result_image = np.zeros_like(img)\n",
    "                cv2.drawContours(result_image, contours, -1, (255, 255, 255), 1)\n",
    "\n",
    "        # 결과 저장\n",
    "        cv2.imwrite(output_image_path, result_image)\n",
    "\n",
    "def apply_of(outfocus_no):\n",
    "    # YOLO 모델 파일 및 구성 파일 설정\n",
    "    yolo_cfg = 'yolov4.cfg'\n",
    "    yolo_weights = 'yolov4.weights'\n",
    "    yolo_names = 'coco.names'\n",
    "\n",
    "    # COCO 클래스 이름 로드\n",
    "    with open(yolo_names, 'r') as f:\n",
    "        classes = f.read().splitlines()\n",
    "\n",
    "    # 네트워크 모델 불러오기\n",
    "    net = cv2.dnn.readNetFromDarknet(yolo_cfg, yolo_weights)\n",
    "    layer_names = net.getLayerNames()\n",
    "\n",
    "    # output_layers 얻기\n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "    # 폴더 정보 로드\n",
    "    input_image_folder = df_FileFolder.Item[0]\n",
    "    output_image_folder = df_FileFolder.Item[1]\n",
    "\n",
    "    # for 반복문\n",
    "    for file in os.listdir(input_image_folder):\n",
    "        # 이미지 로드\n",
    "        input_image_path = os.path.join(input_image_folder, file)\n",
    "        output_image_path = os.path.join(output_image_folder, file)\n",
    "        img = cv2.imread(input_image_path)\n",
    "        height, width, channels = img.shape\n",
    "\n",
    "        # 사람 탐지\n",
    "        blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "        net.setInput(blob)\n",
    "        outs = net.forward(output_layers)\n",
    "\n",
    "        # 탐지된 객체의 정보를 저장할 리스트\n",
    "        class_ids = []\n",
    "        confidences = []\n",
    "        boxes = []\n",
    "\n",
    "        # 탐지된 객체 분석\n",
    "        for out in outs:\n",
    "            for detection in out:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                if class_id == classes.index(\"person\") and confidence > 0.5:\n",
    "                    # 탐지된 객체의 바운딩 박스 좌표 계산\n",
    "                    center_x = int(detection[0] * width)\n",
    "                    center_y = int(detection[1] * height)\n",
    "                    w = int(detection[2] * width)\n",
    "                    h = int(detection[3] * height)\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "        # Non-maximum suppression을 이용한 중복 박스 제거\n",
    "        indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "        match outfocus_no:\n",
    "            case 0 :\n",
    "                # 사람 영역을 마스크로 생성\n",
    "                mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "                for i in indices:\n",
    "                    x, y, w, h = boxes[i]\n",
    "                    mask[y:y+h, x:x+w] = 255\n",
    "\n",
    "                # 마스크를 활용해 사람 영역을 제외한 영역에 블러 효과 적용\n",
    "                blurred_img = cv2.GaussianBlur(img, (21, 21), 0)\n",
    "                output_img = np.where(mask[:, :, np.newaxis] == 255, img, blurred_img)\n",
    "\n",
    "            case 1:\n",
    "                    # 사람 영역을 따라 마스크 생성\n",
    "                mask = np.zeros(img.shape[:2], np.uint8)\n",
    "                bgdModel = np.zeros((1, 65), np.float64)\n",
    "                fgdModel = np.zeros((1, 65), np.float64)\n",
    "\n",
    "                for i in indices:\n",
    "                    x, y, w, h = boxes[i]\n",
    "                    rect = (x, y, w, h)\n",
    "                    cv2.grabCut(img, mask, rect, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_RECT)\n",
    "\n",
    "                # 확실한 배경과 확실한 전경 픽셀을 0 또는 1로 변경\n",
    "                mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')\n",
    "                img_fg = img * mask2[:, :, np.newaxis]\n",
    "\n",
    "                    # 마스크를 활용해 사람 영역을 제외한 영역에 블러 효과 적용\n",
    "                blurred_img = cv2.GaussianBlur(img, (21, 21), 0)\n",
    "                output_img = np.where(mask2[:, :, np.newaxis] == 1, img, blurred_img)\n",
    "\n",
    "        # 결과 이미지 저장\n",
    "        cv2.imwrite(output_image_path, output_img)\n",
    "\n",
    "def apply_af_train():\n",
    "    # 1. 데이터 로드 및 라벨링\n",
    "    data_dir = df_FileFolder.Item[2] # 각 폴더가 들어있는 상위 폴더 경로\n",
    "    \n",
    "    # 현재시간 가져와 결과 파일만들기\n",
    "    now_day = datetime.today().strftime('%Y%m%d')\n",
    "    model_save_path = now_day + '_cnn_model.h5'\n",
    "    img_width, img_height = 150, 150\n",
    "    batch_size = 32\n",
    "    epochs = 50\n",
    "\n",
    "    # 2. 데이터 생성기\n",
    "    datagen = ImageDataGenerator(rescale=1./255, validation_split=0.3)\n",
    "\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training'\n",
    "    )\n",
    "\n",
    "    validation_generator = datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation'\n",
    "    )\n",
    "\n",
    "    # 3. CNN 모델 구성\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dense(len(train_generator.class_indices), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # 4. 모델 학습\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=train_generator.samples // batch_size,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_generator.samples // batch_size,\n",
    "        epochs=epochs\n",
    "    )\n",
    "\n",
    "    # 5. 모델 저장\n",
    "    model.save(model_save_path)\n",
    "    lbPath[3].delete('1.0', tk.END)\n",
    "    lbPath[3].insert(tk.INSERT, chars=model_save_path)\n",
    "    update_Master(3,model_save_path)\n",
    "\n",
    "def apply_af():\n",
    "    # 1. 데이터 로드 및 라벨링\n",
    "    model_save_path = df_FileFolder.Item[3]\n",
    "    \n",
    "    # 6. 새로운 이미지로 예측\n",
    "    test_image_path = df_FileFolder.Item[4]\n",
    "    predict_image(test_image_path, model_save_path)\n",
    "\n",
    "def predict_image(image_path, model_path):\n",
    "    data_dir = df_FileFolder.Item[2] # 각 폴더가 들어있는 상위 폴더 경로\n",
    "    model = load_model(model_path)\n",
    "    img_width, img_height = 150, 150\n",
    "    img = load_img(image_path, target_size=(img_width, img_height))\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    batch_size = 32\n",
    "    \n",
    "    predictions = model.predict(img_array)\n",
    "\n",
    "    datagen = ImageDataGenerator(rescale=1./255, validation_split=0.3)\n",
    "    \n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training'\n",
    "    )\n",
    "\n",
    "    class_indices = train_generator.class_indices\n",
    "    class_labels = list(class_indices.keys())\n",
    "    result_txt = 'Animal Test Result : '\n",
    "\n",
    "    for label, probability in zip(class_labels, predictions[0]):\n",
    "        result_txt = result_txt + '\\n' + f\"{label}: {probability:.4f}\"\n",
    "    \n",
    "    tk.messagebox.showinfo(title = \"Animal Test Result\", message = result_txt)\n",
    "    return predictions[0]\n",
    "\n",
    "\n",
    "### GUI용 함수\n",
    "# GUIMaster Data 업데이트 : \n",
    "def update_Master(idx, var):\n",
    "    df_FileFolder.Item[idx] = var\n",
    "    df_FileFolder.to_csv('GUIMaster.csv', index=False)\n",
    "\n",
    "# 폴더/파일 경로 바꾸는 버튼을 눌렀을때 업데이트\n",
    "def onClick(i, fileYN):\n",
    "    # 폴더 경로 바꾸는 로직 (fineYN = 0 일때)\n",
    "    if fileYN == 0:\n",
    "        folder_selected = filedialog.askdirectory()\n",
    "        var = folder_selected\n",
    "    # 파일 경로 바꾸는 로직 (fineYN = 0 이 아닐때)\n",
    "    else:\n",
    "        folder_selected = filedialog.askopenfile()\n",
    "        var = folder_selected.name\n",
    "\n",
    "    lbPath[i].delete('1.0', tk.END)\n",
    "    lbPath[i].insert(tk.INSERT, chars=var)\n",
    "    update_Master(i,var)\n",
    "    \n",
    "## Main Code\n",
    "# GUI 구성\n",
    "win = tk.Tk()\n",
    "win.geometry(Gsize)\n",
    "win.title('Python image editor')\n",
    "\n",
    "# Frame 설정하기\n",
    "frameF = tk.Frame(win, pady=5, width = 590, padx = 5)\n",
    "frameF.grid(row=0, column=0, sticky= \"ew\", padx=5,pady=5)\n",
    "frameNoise = tk.LabelFrame(win, text=\"Remove Noise\", pady=5, width = 590, padx = 5)\n",
    "frameNoise.grid(row=1, column=0, sticky= \"ew\", padx=5, pady=5)\n",
    "frameEdge = tk.LabelFrame(win, text=\"Edge Detection\", pady=5, width = 590, padx = 5)\n",
    "frameEdge.grid(row=2, column=0, sticky= \"ew\", padx=5, pady=5)\n",
    "frameOutFocus = tk.LabelFrame(win, text=\"Human reinforce\", pady=5, width = 590, padx = 5)\n",
    "frameOutFocus.grid(row=3, column=0, sticky= \"ew\", padx=5, pady=5)\n",
    "frameAnimalFace = tk.LabelFrame(win, text=\"Animal Face\", pady=5, width = 590, padx = 5)\n",
    "frameAnimalFace.grid(row=4, column=0, sticky= \"ew\", padx=5, pady=5)\n",
    "\n",
    "# 폴더 경로 설정 GUI\n",
    "lbFame = []\n",
    "lbPath = []\n",
    "btnPath =[]\n",
    "\n",
    "for i,x in enumerate(label_f):\n",
    "    match i:\n",
    "        case 0 | 1 :\n",
    "            lbFame.append(tk.Label(frameF, text=x, width=15,padx =5, pady = 5))\n",
    "            lbPath.append(tk.Text(frameF, width = 50, height = 1, padx =5, pady = 5, background='lightgrey'))\n",
    "            btnPath.append(tk.Button(frameF, text=\"Change Path\", width=10, padx =5, pady = 5, command=lambda i=i: onClick(i,fileyn[i])))\n",
    "        case 2 | 3 | 4 :\n",
    "            lbFame.append(tk.Label(frameAnimalFace, text=x, width=15, padx =5, pady = 5))\n",
    "            lbPath.append(tk.Text(frameAnimalFace, width = 50, height = 1, padx =5, pady = 5, background='lightgrey'))\n",
    "            btnPath.append(tk.Button(frameAnimalFace, text=\"Change Path\", width=10, padx =5, pady = 5, command=lambda i=i: onClick(i,fileyn[i])))\n",
    "\n",
    "    # 폴더/파일 이름 초기값 넣기\n",
    "    lbPath[i].insert(tk.INSERT, chars=df_FileFolder.Item[i])\n",
    "\n",
    "    lbFame[i].grid(row=i, column=0, padx =5, sticky=tk.W)\n",
    "    lbPath[i].grid(row=i, column=1, padx =5, sticky=tk.W)\n",
    "    btnPath[i].grid(row=i, column=2, padx =5, sticky=tk.W)\n",
    "\n",
    "# Noise Frame 기능 구현\n",
    "Filter_var = tk.IntVar()\n",
    "Filter_var.set(0)\n",
    "radio1 = tk.Radiobutton(frameNoise, text=\"Gaussian\", variable=Filter_var, width = 17, padx =1, value=0)\n",
    "radio2 = tk.Radiobutton(frameNoise, text=\"Median\", variable=Filter_var,width = 17, padx = 1, value=1)\n",
    "radio3 = tk.Radiobutton(frameNoise, text=\"Bilateral\", variable=Filter_var,width = 17, padx = 3, value=2)\n",
    "radio1.grid(row=0, column=0, sticky=tk.W)\n",
    "radio2.grid(row=0, column=1, sticky=tk.W)\n",
    "radio3.grid(row=0, column=2, sticky=tk.W)\n",
    "radio1.select()\n",
    "radio2.deselect()\n",
    "button_filter = tk.Button(frameNoise, text=\"Apply Filter\",width = 18, padx=5, command=lambda: apply_filter(Filter_var.get()))\n",
    "button_filter.grid(row=0, column=3, sticky=tk.E)\n",
    "\n",
    "# Edge Frame 기능 구현\n",
    "Edge_var = tk.IntVar()\n",
    "Edge_var.set(0)\n",
    "radio4 = tk.Radiobutton(frameEdge, text=\"Canny\", variable=Edge_var,width = 28, padx=1, value=0)\n",
    "radio5 = tk.Radiobutton(frameEdge, text=\"Contour\", variable=Edge_var, width = 28, padx = 1, value=1)\n",
    "radio4.grid(row=0, column=0, sticky=tk.W)\n",
    "radio5.grid(row=0, column=1, sticky=tk.W)\n",
    "radio4.select()\n",
    "radio5.deselect()\n",
    "button_edge = tk.Button(frameEdge, text=\"Apply Edge\", width = 18, padx=3, command=lambda: apply_edge(Edge_var.get()))\n",
    "button_edge.grid(row=0, column=2, sticky=tk.E)\n",
    "\n",
    "# OutFocus Frame 기능 구현\n",
    "OF_var = tk.IntVar()\n",
    "OF_var.set(0)\n",
    "radio6 = tk.Radiobutton(frameOutFocus, text=\"Rectangle\", variable=OF_var, width = 28, padx = 1, value=0)\n",
    "radio7 = tk.Radiobutton(frameOutFocus, text=\"Boundary\", variable=OF_var,width = 28, padx = 1, value=1)\n",
    "radio6.grid(row=0, column=0, sticky=tk.W)\n",
    "radio7.grid(row=0, column=1, sticky=tk.W)\n",
    "radio6.select()\n",
    "radio7.deselect()\n",
    "button_of = tk.Button(frameOutFocus, text=\"Apply OutFocus\", width = 18, padx = 3, command=lambda: apply_of(OF_var.get()))\n",
    "button_of.grid(row=0, column=2, sticky=tk.E)\n",
    "\n",
    "# Animal Face Frame 기능 구현\n",
    "button_train = tk.Button(frameAnimalFace, text=\"Training\", command=lambda: apply_af_train())\n",
    "button_train.grid(row=5, column=0, sticky=tk.E, pady = 10) \n",
    "button_af = tk.Button(frameAnimalFace, text=\"Apply Animal Face\", command=lambda: apply_af())\n",
    "button_af.grid(row=5, column=1, sticky=tk.E)\n",
    "\n",
    "win.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
